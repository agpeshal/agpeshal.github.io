<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous"/><meta name="description" content="My name is Peshal Agarwal. I am a final year student at ETH Zurich. "/><title>Publications | Peshal Agarwal ETH Zurich</title><meta name="next-head-count" content="5"/><link rel="preload" href="/_next/static/css/381f5b9c92d1673af027.css" as="style"/><link rel="stylesheet" href="/_next/static/css/381f5b9c92d1673af027.css" data-n-g=""/><link rel="preload" href="/_next/static/css/94afa6f8c89b3534921a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/94afa6f8c89b3534921a.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/webpack-5c24d1dda7c382c52376.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.0021446505933bd5cafd.js" as="script"/><link rel="preload" href="/_next/static/chunks/f6078781a05fe1bcb0902d23dbbb2662c8d200b3.d2eac0450a3f8665534b.js" as="script"/><link rel="preload" href="/_next/static/chunks/main-29b9ab465aa5e10b249d.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-0e68a1950cfb598e2db6.js" as="script"/><link rel="preload" href="/_next/static/chunks/096d9f7848fe1a4ad64d43dd8449f92a30633aee.1501ade37eb8b30c78b9.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/publications-52d60723c6bc1ffd72f0.js" as="script"/></head><body><div id="__next"><div class="Publications_publications__3naGt"><h1>SnapBoost: A Heterogeneous Boosting Machine</h1><h3>Abstract</h3><p>Modern gradient boosting software frameworks, such as XGBoost and LightGBM, implement Newton descent in a functional space. At each boosting iteration, their goal is to find the base hypothesis, selected from some base hypothesis class, that is closest to the Newton descent direction in a Euclidean sense. Typically, the base hypothesis class is fixed to be all binary decision trees up to a given depth. In this work, we study a Heterogeneous Newton Boosting Machine (HNBM) in which the base hypothesis class may vary across boosting iterations. Specifically, at each boosting iteration, the base hypothesis class is chosen, from a fixed set of subclasses, by sampling from a probability distribution. We derive a global linear convergence rate for the HNBM under certain assumptions, and show that it agrees with existing rates for Newtonâ€™s method when the Newton direction can be perfectly fitted by the base hypothesis at each boosting iteration. We then describe a particular realization of a HNBM, SnapBoost, that, at each boosting iteration, randomly selects between either a decision tree of variable depth or a linear regressor with random Fourier features. We describe how SnapBoost is implemented, with a focus on the training complexity. Finally, we present experimental results, using OpenML and Kaggle datasets, that show that SnapBoost is able to achieve better generalization loss than competing boosting frameworks, without taking significantly longer to tune</p><a href="../assets/pdf/paper.pdf" download="SnapBoost: A Heterogeneous Boosting Machine"><button class="MuiButtonBase-root MuiButton-root MuiButton-contained MuiButton-containedPrimary" tabindex="0" type="button"><span class="MuiButton-label">READ MORE</span></button></a></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/publications","query":{},"buildId":"TpwGJZT-7bqLeLmCs6KGO","nextExport":true,"autoExport":true,"isFallback":false}</script><script nomodule="" src="/_next/static/chunks/polyfills-f9f7258084ca4d945e7b.js"></script><script src="/_next/static/chunks/webpack-5c24d1dda7c382c52376.js" async=""></script><script src="/_next/static/chunks/framework.0021446505933bd5cafd.js" async=""></script><script src="/_next/static/chunks/f6078781a05fe1bcb0902d23dbbb2662c8d200b3.d2eac0450a3f8665534b.js" async=""></script><script src="/_next/static/chunks/main-29b9ab465aa5e10b249d.js" async=""></script><script src="/_next/static/chunks/pages/_app-0e68a1950cfb598e2db6.js" async=""></script><script src="/_next/static/chunks/096d9f7848fe1a4ad64d43dd8449f92a30633aee.1501ade37eb8b30c78b9.js" async=""></script><script src="/_next/static/chunks/pages/publications-52d60723c6bc1ffd72f0.js" async=""></script><script src="/_next/static/TpwGJZT-7bqLeLmCs6KGO/_buildManifest.js" async=""></script><script src="/_next/static/TpwGJZT-7bqLeLmCs6KGO/_ssgManifest.js" async=""></script></body></html>